{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ac3d6d",
   "metadata": {
    "id": "65ac3d6d"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72a9479-ea54-443d-a15b-99507017657f",
   "metadata": {},
   "source": [
    "### PreProcessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cad5e380-de0f-4476-a004-8bec5ae54f77",
   "metadata": {},
   "source": [
    "#### Data Preparation: Importing Libraries and Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "598a9eb3-77ca-4496-a81c-da626e2d7575",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Libraries ##\n",
    "\n",
    "import pandas as pd                    # For data manipulation and analysis\n",
    "import matplotlib.pyplot as plt        # For data visualization\n",
    "import seaborn as sns                  # For enhanced data visualization\n",
    "from sklearn.model_selection import train_test_split  # For data splitting\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  # For text feature extraction\n",
    "import nltk                            # Natural Language Toolkit for text processing\n",
    "from nltk.stem import WordNetLemmatizer  # Lemmatization for text data\n",
    "from nltk.corpus import stopwords      # Stopword removal for text data\n",
    "from nltk.stem import PorterStemmer     # Text stemming for text data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler  # Data scaling\n",
    "from sklearn.feature_selection import f_regression, SelectKBest, f_classif  # Feature selection\n",
    "from sklearn.metrics import r2_score    # Evaluation metric for regression models\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV  # Hyperparameter tuning\n",
    "import string                          # String manipulation functions\n",
    "import transformers\n",
    "from sklearn.decomposition import PCA  # Import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73cc4a66-baa5-4589-8047-1e648f1fdb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haesunjung/anaconda3/envs/nlp_modelling/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3553: DtypeWarning: Columns (9,10,12,13,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "## Loading the Dataset ##\n",
    "\n",
    "file_path = '../../Datasets/cleaned_data.csv'\n",
    "real_estate = pd.read_csv(file_path)\n",
    "\n",
    "# A quick check by sampling 10 rows from the 'real_estate' DataFrame\n",
    "real_estate = real_estate.sample(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551823c3-a123-42bc-b90b-15139c8e77c0",
   "metadata": {},
   "source": [
    "Checking the Initial Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ced6d66-70c5-46a3-8b0d-6931833f1185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10000 entries, 665415 to 365264\n",
      "Data columns (total 20 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed: 0        10000 non-null  int64  \n",
      " 1   Serial Number     10000 non-null  int64  \n",
      " 2   List Year         10000 non-null  int64  \n",
      " 3   Date Recorded     10000 non-null  object \n",
      " 4   Town              10000 non-null  object \n",
      " 5   Address           10000 non-null  object \n",
      " 6   Assessed Value    10000 non-null  float64\n",
      " 7   Sale Amount       10000 non-null  float64\n",
      " 8   Sales Ratio       10000 non-null  float64\n",
      " 9   Property Type     6193 non-null   object \n",
      " 10  Residential Type  6137 non-null   object \n",
      " 11  Non Use Code      2425 non-null   float64\n",
      " 12  Assessor Remarks  1535 non-null   object \n",
      " 13  OPM remarks       90 non-null     object \n",
      " 14  Location          1971 non-null   object \n",
      " 15  Full Address      9847 non-null   object \n",
      " 16  latitude          4907 non-null   float64\n",
      " 17  longitude         4907 non-null   float64\n",
      " 18  month_recorded    10000 non-null  float64\n",
      " 19  year_recorded     10000 non-null  float64\n",
      "dtypes: float64(8), int64(3), object(9)\n",
      "memory usage: 1.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Dataset Overview\n",
    "\n",
    "print(real_estate.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4035110b-d5fd-4da9-bd7a-9977bd403e56",
   "metadata": {},
   "source": [
    "Dataset Overview:\n",
    "- The dataset contains a substantial 997,193 entries across 20 columns.\n",
    "- The columns encompass a variety of data types, such as integers, floats, and objects.\n",
    "- It's important to note that several columns have missing values, as indicated by non-null counts.\n",
    "- The total memory usage for this dataset is approximately 152.2 MB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93118203-5dd3-4217-bd9e-14f6a47cb231",
   "metadata": {},
   "source": [
    "#### Streamlining the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d389b93-5aea-44b6-be81-4c1bbe70ec54",
   "metadata": {},
   "source": [
    "Columns to be dropped:\n",
    "- 'Unnamed: 0': unnecessary column from Geocoding\n",
    "- 'Serial Number': not applicable in modelling\n",
    "- 'Date Recorded': feature-engineered as month and year\n",
    "- 'Sales Ratio': since it is a ratio between assessed and sales values, it would cause data-leakage\n",
    "- 'Address' & 'Full Address': used in geocoding, not applicable in modelling\n",
    "- 'Property Type': Redundant with \"Residential Type\"\n",
    "- 'Location': feature-engineered as longitude and latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d1ca1c2-bbd3-46b2-950d-e4e7f1155ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10000 entries, 665415 to 365264\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   List Year         10000 non-null  int64  \n",
      " 1   Town              10000 non-null  object \n",
      " 2   Assessed Value    10000 non-null  float64\n",
      " 3   Sale Amount       10000 non-null  float64\n",
      " 4   Residential Type  6137 non-null   object \n",
      " 5   Non Use Code      2425 non-null   float64\n",
      " 6   Assessor Remarks  1535 non-null   object \n",
      " 7   OPM remarks       90 non-null     object \n",
      " 8   latitude          4907 non-null   float64\n",
      " 9   longitude         4907 non-null   float64\n",
      " 10  month_recorded    10000 non-null  float64\n",
      " 11  year_recorded     10000 non-null  float64\n",
      "dtypes: float64(7), int64(1), object(4)\n",
      "memory usage: 1015.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3281 entries, 542698 to 931958\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   List Year         3281 non-null   int64  \n",
      " 1   Town              3281 non-null   object \n",
      " 2   Assessed Value    3281 non-null   float64\n",
      " 3   Sale Amount       3281 non-null   float64\n",
      " 4   Residential Type  3281 non-null   object \n",
      " 5   Non Use Code      1767 non-null   float64\n",
      " 6   Assessor Remarks  1243 non-null   object \n",
      " 7   OPM remarks       82 non-null     object \n",
      " 8   latitude          3281 non-null   float64\n",
      " 9   longitude         3281 non-null   float64\n",
      " 10  month_recorded    3281 non-null   float64\n",
      " 11  year_recorded     3281 non-null   float64\n",
      "dtypes: float64(7), int64(1), object(4)\n",
      "memory usage: 333.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Drop the following unnecessary columns\n",
    "\n",
    "real_estate_nonull = real_estate.drop(columns=['Unnamed: 0', 'Serial Number', 'Date Recorded', 'Sales Ratio', 'Address', 'Full Address', 'Property Type', 'Location'], axis=1)\n",
    "\n",
    "# Check the DataFrame's structure after removing the columns\n",
    "\n",
    "print(real_estate_nonull.info())\n",
    "\n",
    "# Remove rows with missing values in the 'latitude' and 'Residential Type' columns\n",
    "real_estate_nonull = real_estate_nonull.dropna(subset=[\"latitude\", \"Residential Type\"])\n",
    "\n",
    "# Check the DataFrame's structure after dropping rows\n",
    "print(real_estate_nonull.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40193995-8216-4dd1-98f2-dfae8ef03e64",
   "metadata": {},
   "source": [
    "#### Filling Missing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475fdb67-6fa6-4c18-84fa-58bdbe9070e9",
   "metadata": {},
   "source": [
    "Address missing values in text columns by inserting \"na.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8652cd0b-583c-45d9-a413-adc47de97c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in text columns with \"na\"\n",
    "\n",
    "real_estate_nonull = real_estate_nonull.fillna(\"na\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ca12d-586c-4a33-bbbc-a9a8dbc05719",
   "metadata": {},
   "source": [
    "Examine the 'real_estate_nonull' DataFrame to ensure that the columns are dropped and \"na\" is inserted in place of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "340acc0a-bf7c-48b9-9cd6-2ed349aa9269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        List Year       Town  Assessed Value  Sale Amount Residential Type  \\\n",
      "750015       2008  Waterbury        146790.0      90000.0     Three Family   \n",
      "891853       2016     Berlin        121200.0     204000.0    Single Family   \n",
      "735443       2007   Cheshire        213860.0     390000.0    Single Family   \n",
      "496942       2018    Shelton        210280.0     329500.0            Condo   \n",
      "510576       2019   Hartford        163065.0     582500.0    Single Family   \n",
      "\n",
      "       Non Use Code Assessor Remarks OPM remarks   latitude  longitude  \\\n",
      "750015         14.0               na          na  41.546869 -73.046510   \n",
      "891853           na   qualified sale          na -72.797350  41.590300   \n",
      "735443           na               na          na -72.853030  41.535790   \n",
      "496942           na               na          na  47.627007   2.177747   \n",
      "510576           na               na          na  41.784468 -72.710912   \n",
      "\n",
      "        month_recorded  year_recorded  \n",
      "750015             9.0         2009.0  \n",
      "891853             6.0         2017.0  \n",
      "735443            10.0         2007.0  \n",
      "496942             5.0         2019.0  \n",
      "510576             1.0         2020.0  \n"
     ]
    }
   ],
   "source": [
    "# Sample 5 random rows from the DataFrame\n",
    "\n",
    "sample_rows = real_estate_nonull.sample(5)\n",
    "print(sample_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec7b799-5849-46c1-9ea6-14e2611cd79c",
   "metadata": {},
   "source": [
    "Missing Values Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dffdf595-764e-4f21-8306-a4e77bcfbf21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3281 entries, 542698 to 931958\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   List Year         3281 non-null   int64  \n",
      " 1   Town              3281 non-null   object \n",
      " 2   Assessed Value    3281 non-null   float64\n",
      " 3   Sale Amount       3281 non-null   float64\n",
      " 4   Residential Type  3281 non-null   object \n",
      " 5   Non Use Code      3281 non-null   object \n",
      " 6   Assessor Remarks  3281 non-null   object \n",
      " 7   OPM remarks       3281 non-null   object \n",
      " 8   latitude          3281 non-null   float64\n",
      " 9   longitude         3281 non-null   float64\n",
      " 10  month_recorded    3281 non-null   float64\n",
      " 11  year_recorded     3281 non-null   float64\n",
      "dtypes: float64(6), int64(1), object(5)\n",
      "memory usage: 333.2+ KB\n",
      ".info of All Columns\n",
      " None\n",
      "Null Values in Total\n",
      " List Year           0\n",
      "Town                0\n",
      "Assessed Value      0\n",
      "Sale Amount         0\n",
      "Residential Type    0\n",
      "Non Use Code        0\n",
      "Assessor Remarks    0\n",
      "OPM remarks         0\n",
      "latitude            0\n",
      "longitude           0\n",
      "month_recorded      0\n",
      "year_recorded       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# To confirm that all text columns have been filled with \"na,\" check the DataFrame's info.\n",
    "\n",
    "print(\".info of All Columns\\n\", real_estate_nonull.info())\n",
    "print(\"Null Values in Total\\n\", real_estate_nonull.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723b3c02-c8d8-4dbf-9a4b-a544965dd05d",
   "metadata": {},
   "source": [
    "#### Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f19b43-f650-4e33-860d-ce38986289df",
   "metadata": {},
   "source": [
    "##### 1. Feature Matrix and Target Extraction: create the feature matrix 'X' and extract the target variable 'y.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38aadc61-aa53-4cc6-80a6-faf439fbc2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature matrix 'X' by dropping the 'Sale Amount' column\n",
    "X = real_estate_nonull.drop('Sale Amount', axis=1)\n",
    "\n",
    "# Extract the target variable 'y'\n",
    "y = real_estate_nonull[\"Sale Amount\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c542250-9736-4c99-8bfa-b7a024c725c1",
   "metadata": {},
   "source": [
    "##### 2. One-hot Encoding Categorical Columns: perform one-hot encoding on categorical columns to prepare the data for modeling.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bf49bd7-c5a7-45b9-83b8-e5269f9612a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on categorical columns in X\n",
    "X_coded = pd.get_dummies(X, columns=['List Year', 'Town', 'Residential Type', 'Non Use Code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feb828d-40a7-4b2a-80e2-70b506ae9437",
   "metadata": {},
   "source": [
    "##### 3. Data Splitting into Training, Validation, and Test Sets: split into well-defined training, validation, and test sets for model evaluation and generalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4819ca-65bd-41a0-82f0-b81cb239c4e6",
   "metadata": {},
   "source": [
    "Split the data into well-defined training, validation, and test sets to ensure reliable model evaluation and generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c24d093d-f3d9-4bb5-9a7c-8cda4bc03dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "X_rem, X_test, y_rem, y_test = train_test_split(X_coded, y, test_size=0.3, random_state=22)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_rem, y_rem, test_size=0.5, random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b31f9ab-86d0-431a-a888-a8ba41ab4a9e",
   "metadata": {},
   "source": [
    "4. Splitting Features and Printing Shapes (Text and Numerical Data Separation): the division allows for specialized preprocessing of different feature types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98466fa-bf0c-4437-ab16-3d30a1651add",
   "metadata": {},
   "source": [
    "Split the features into text and numerical components for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77dadcd9-6842-460f-b892-3b372073a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_Assessor_Remarks = X_train['Assessor Remarks']\n",
    "X_train_OPM_remarks = X_train['OPM remarks']\n",
    "X_train_numerical = X_train[[\"Assessed Value\", \"latitude\", \"longitude\"]]\n",
    "X_train_coded = X_train.drop([\"Assessor Remarks\", \"OPM remarks\", \"Assessed Value\", \"latitude\", \"longitude\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f06ab8-7d44-4a59-ae8c-40331a99f18a",
   "metadata": {},
   "source": [
    "Repeat the above steps for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "823c6301-8581-4835-9b61-90a162814a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_Assessor_Remarks = X_val['Assessor Remarks']\n",
    "X_val_OPM_remarks = X_val['OPM remarks']\n",
    "X_val_numerical = X_val[[\"Assessed Value\", \"latitude\", \"longitude\"]]\n",
    "X_val_coded = X_val.drop([\"Assessor Remarks\", \"OPM remarks\", \"Assessed Value\", \"latitude\", \"longitude\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecdf30a-fb28-4d84-a24e-eab7a45ab88b",
   "metadata": {},
   "source": [
    "Define a function to split features into text, numerical, and coded components and print their shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbddd91b-15fb-403d-8545-2afd20be8813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features(dataset, prefix, text_features, numerical_features):\n",
    "    text_data = dataset[text_features]\n",
    "    numerical_data = dataset[numerical_features]\n",
    "    coded_data = dataset.drop(text_features + numerical_features, axis=1)\n",
    "    # Extract text and numerical data\n",
    "    print(f\"{prefix}_Assessor_Remarks.shape: {text_data.shape}\")\n",
    "    print(f\"{prefix}_OPM_remarks.shape: {text_data.shape}\")\n",
    "    print(f\"{prefix}_numerical.shape: {numerical_data.shape}\")\n",
    "    print(f\"{prefix}_coded.shape: {coded_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0273ed58-6b09-4e75-a1a5-f30abbc75103",
   "metadata": {},
   "source": [
    "Define the text and numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abd25353-da3c-485b-b910-ea6d72758360",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = [\"Assessor Remarks\", \"OPM remarks\"]\n",
    "numerical_features = [\"Assessed Value\", \"latitude\", \"longitude\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5089b6d-14ab-470a-98ef-1fe6db598371",
   "metadata": {},
   "source": [
    "Call the function for both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "902aa599-012f-4357-9181-9ef71d9212b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Assessor_Remarks.shape: (1148, 2)\n",
      "train_OPM_remarks.shape: (1148, 2)\n",
      "train_numerical.shape: (1148, 3)\n",
      "train_coded.shape: (1148, 213)\n",
      "val_Assessor_Remarks.shape: (1148, 2)\n",
      "val_OPM_remarks.shape: (1148, 2)\n",
      "val_numerical.shape: (1148, 3)\n",
      "val_coded.shape: (1148, 213)\n"
     ]
    }
   ],
   "source": [
    "split_features(X_train, 'train', text_features, numerical_features)\n",
    "split_features(X_val, 'val', text_features, numerical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf90c12c-2b99-4a58-98c1-1624a9d57361",
   "metadata": {},
   "source": [
    "Checking the shape of each dataframe is important for validation and quality control:\n",
    "- It helps ensure that the data splitting process has been executed correctly.- It allows us to verify that the data dimensions align with our expectations.\n",
    "- It allows us to verify that the data dimensions align with our expectations.\n",
    "- Identifying unexpected shapes can be an early indicator of data issues or errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c4885a5-4c9e-42b2-b809-d1701c548c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train text shape: (1148, 2)\n",
      "X_val text shape: (1148, 2)\n",
      "X_train numerical shape: (1148, 3)\n",
      "X_val numerical shape: (1148, 3)\n",
      "X_train_coded shape: (1148, 213)\n",
      "X_val_coded shape: (1148, 213)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of each dataframe:\n",
    "print(f\"X_train text shape: {X_train[text_features].shape}\")\n",
    "print(f\"X_val text shape: {X_val[text_features].shape}\")\n",
    "print(f\"X_train numerical shape: {X_train[numerical_features].shape}\")\n",
    "print(f\"X_val numerical shape: {X_val[numerical_features].shape}\")\n",
    "print(f\"X_train_coded shape: {X_train_coded.shape}\")\n",
    "print(f\"X_val_coded shape: {X_val_coded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c03ff02-99b1-4aec-a7c8-0f44b8751c1b",
   "metadata": {},
   "source": [
    "#### Feature Engineering for Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8924fa53-00d7-4740-94ba-282fd4ea7052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/haesunjung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/haesunjung/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/haesunjung/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering for Natural Language Processing\n",
    "\n",
    "## 1. Create Tokenizer\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Define a custom tokenizer function\n",
    "from nltk.stem import PorterStemmer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "def custom_tokenizer(s):\n",
    "    # Remove punctuation\n",
    "    for char in s:\n",
    "        if char in string.punctuation:\n",
    "            s = s.replace(char, '')\n",
    "\n",
    "    # Make the string lowercase\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Split the string at each space\n",
    "    tokens = s.split()\n",
    "\n",
    "    # Create a set of NLTK stop words for faster lookup\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Stem tokens and filter out stop words\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
    "\n",
    "    return stemmed_tokens\n",
    "    \n",
    "# Initialize a BERT tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def custom_tokenizer_bert(s, max_length=64):  # Add 'max_length' parameter\n",
    "    # Tokenize the input text using the BERT tokenizer\n",
    "    tokens = bert_tokenizer(s, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "    # Convert tokens to numerical IDs\n",
    "    input_ids = tokens['input_ids']\n",
    "    attention_mask = tokens['attention_mask']\n",
    "\n",
    "    return input_ids, attention_mask  # Return both input_ids and attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a104cba4-b5b3-4047-b448-c98a832704db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "## 2. Instantiate CountVectorizer, TfidfVectorizer, and BERT Model\n",
    "# Initialize CountVectorizer, TfidfVectorizer, and BERT model\n",
    "count_vectorizer = CountVectorizer(\n",
    "    tokenizer=custom_tokenizer,\n",
    "    stop_words='english',\n",
    "    min_df=10,\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    tokenizer=custom_tokenizer,\n",
    "    stop_words='english',\n",
    "    min_df=10,\n",
    "    max_features=1000\n",
    ")\n",
    "\n",
    "# Load a pre-trained BERT model\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d70e36a-77b8-4220-9051-3cf70155e1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haesunjung/anaconda3/envs/nlp_modelling/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afterward', 'alon', 'alreadi', 'alway', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becom', 'besid', 'cri', 'describ', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'otherwis', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'thenc', 'thereaft', 'therebi', 'therefor', 'thu', 'togeth', 'twelv', 'twenti', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev'] not in stop_words.\n",
      "  % sorted(inconsistent)\n"
     ]
    }
   ],
   "source": [
    "## 3. Vectorize & Fit the Values\n",
    "# Vectorize text data\n",
    "\n",
    "# For Assessor Remarks (using CountVectorizer)\n",
    "X_train_Assessor_Remarks_vectorized = count_vectorizer.fit_transform(X_train_Assessor_Remarks.values)\n",
    "X_val_Assessor_Remarks_vectorized = count_vectorizer.transform(X_val_Assessor_Remarks.values)\n",
    "\n",
    "\n",
    "X_train_Assessor_Remarks_df = pd.DataFrame(X_train_Assessor_Remarks_vectorized.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "X_val_Assessor_Remarks_df = pd.DataFrame(X_val_Assessor_Remarks_vectorized.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "# For OPM Remarks (using BERT)\n",
    "# Tokenize and encode OPM Remarks using BERT\n",
    "max_length = 32  # Define the maximum sequence length\n",
    "X_train_OPM_remarks_tokenized = X_train_OPM_remarks.apply(lambda x: custom_tokenizer_bert(x, max_length))\n",
    "X_val_OPM_remarks_tokenized = X_val_OPM_remarks.apply(lambda x: custom_tokenizer_bert(x, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9171396e-d329-4805-8404-6ee6ac4e43c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized data to tensors\n",
    "X_train_OPM_remarks_input = torch.stack([item[0] for item in X_train_OPM_remarks_tokenized])\n",
    "X_train_OPM_remarks_mask = torch.stack([item[1] for item in X_train_OPM_remarks_tokenized])\n",
    "X_val_OPM_remarks_input = torch.stack([item[0] for item in X_val_OPM_remarks_tokenized])\n",
    "X_val_OPM_remarks_mask = torch.stack([item[1] for item in X_val_OPM_remarks_tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "802d4f6e-14b1-4b82-b3a7-043958b68366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_OPM_remarks_input shape: torch.Size([1148, 1, 32])\n",
      "X_train_OPM_remarks_mask shape: torch.Size([1148, 1, 32])\n",
      "X_train_OPM_remarks_input data type: torch.int64\n",
      "X_train_OPM_remarks_mask data type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_OPM_remarks_input shape:\", X_train_OPM_remarks_input.shape)\n",
    "print(\"X_train_OPM_remarks_mask shape:\", X_train_OPM_remarks_mask.shape)\n",
    "print(\"X_train_OPM_remarks_input data type:\", X_train_OPM_remarks_input.dtype)\n",
    "print(\"X_train_OPM_remarks_mask data type:\", X_train_OPM_remarks_mask.dtype)\n",
    "\n",
    "# Remove the extra dimension with .squeeze(1)\n",
    "X_train_OPM_remarks_input = X_train_OPM_remarks_input.squeeze(1)\n",
    "X_train_OPM_remarks_mask = X_train_OPM_remarks_mask.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "356a8037-4233-415c-af5d-039fa8e2ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Assuming you have the tensors 'X_train_OPM_remarks_input' and 'X_train_OPM_remarks_mask' defined\n",
    "# You can create a dataset like this:\n",
    "dataset = TensorDataset(X_train_OPM_remarks_input, X_train_OPM_remarks_mask)\n",
    "\n",
    "# Then, you can use the DataLoader as shown in your code:\n",
    "batch_size = 16\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Now, the 'dataset' variable is defined and can be used in the DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22808eb8-ad02-437c-ba00-327f0f572d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings from the model's output\n",
    "X_train_OPM_remarks_embeddings = []\n",
    "\n",
    "# Iterate through the data in batches\n",
    "for batch in dataloader:\n",
    "    input_ids, attention_mask = batch\n",
    "    \n",
    "    # Process the batch with BERT\n",
    "    batch_outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Extract embeddings for this batch\n",
    "    batch_embeddings = batch_outputs.last_hidden_state\n",
    "    \n",
    "    # Append the batch embeddings to the list\n",
    "    X_train_OPM_remarks_embeddings.append(batch_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad2402aa-7e54-4200-b677-a07a8518e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the batch embeddings to obtain the embeddings for the entire dataset\n",
    "X_train_OPM_remarks_embeddings = torch.cat(X_train_OPM_remarks_embeddings, dim=0)\n",
    "\n",
    "# Calculate the mean of embeddings along the sequence length\n",
    "X_train_OPM_remarks_vectorized = X_train_OPM_remarks_embeddings.mean(dim=1).detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8371a635-ad54-4389-a5e5-99eea1accab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Assuming you have the tensors 'X_train_OPM_remarks_input' and 'X_train_OPM_remarks_mask' defined\n",
    "# You can create a dataset like this:\n",
    "val_dataset = TensorDataset(X_val_OPM_remarks_input, X_val_OPM_remarks_mask)\n",
    "\n",
    "# Then, you can use the DataLoader as shown in your code:\n",
    "batch_size = 8\n",
    "val_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f56c372f-46e3-45d4-a50d-08c8fa37ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now, the 'dataset' variable is defined and can be used in the DataLoader.\n",
    "\n",
    "# Assuming you have a DataLoader for validation data named 'val_dataloader'\n",
    "X_val_OPM_remarks_embeddings = []\n",
    "\n",
    "# Iterate through the data in batches for validation data\n",
    "for batch in val_dataloader:\n",
    "    input_ids, attention_mask = batch\n",
    "    \n",
    "    # Process the batch with BERT for validation data\n",
    "    batch_outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Extract embeddings for this batch\n",
    "    batch_embeddings = batch_outputs.last_hidden_state\n",
    "    \n",
    "    # Append the batch embeddings to the list for validation data\n",
    "    X_val_OPM_remarks_embeddings.append(batch_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "477bf8e0-9b1c-4054-944c-3ab7cad7c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate the batch embeddings for the entire validation dataset\n",
    "X_val_OPM_remarks_embeddings = torch.cat(X_val_OPM_remarks_embeddings, dim=0)\n",
    "\n",
    "# Calculate the mean of embeddings along the sequence length for validation data\n",
    "X_val_OPM_remarks_vectorized = X_val_OPM_remarks_embeddings.mean(dim=1).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cca6769-d55b-434a-9dac-55408eb06a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming X_val_OPM_remarks_vectorized is a numpy array\n",
    "data = X_val_OPM_remarks_vectorized  # Replace with your actual data\n",
    "\n",
    "# Create a DataFrame with columns if you have them\n",
    "# For example, you can create columns like 'feature_1', 'feature_2', 'feature_3', etc.\n",
    "column_names = [f'feature_{i}' for i in range(X_train_OPM_remarks_vectorized.shape[1])]\n",
    "\n",
    "X_train_OPM_remarks_df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "# Now, df is a DataFrame containing your vectorized data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cf39639-309e-45c7-ad0b-da8e53f57a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming X_val_OPM_remarks_vectorized is a numpy array\n",
    "data = X_val_OPM_remarks_vectorized  # Replace with your actual data\n",
    "\n",
    "# Create a DataFrame with columns if you have them\n",
    "# For example, you can create columns like 'feature_1', 'feature_2', 'feature_3', etc.\n",
    "column_names = [f'feature_{i}' for i in range(X_val_OPM_remarks_vectorized.shape[1])]\n",
    "\n",
    "X_val_OPM_remarks_df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "# Now, df is a DataFrame containing your vectorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "505893e0-b824-4a25-a649-2e1d5efbc9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Scaling Numerical Data (Scaling is important for various machine learning algorithms)\n",
    "# For scaling numerical data, you can use StandardScaler, MinMaxScaler, or RobustScaler.\n",
    "\n",
    "# Example using StandardScaler:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_numerical_scaled = scaler.fit_transform(X_train_numerical)\n",
    "X_val_numerical_scaled = scaler.transform(X_val_numerical)\n",
    "\n",
    "# Scaling numerical data is essential in machine learning for reasons like maintaining consistent units and preventing features with large values from dominating the model.\n",
    "\n",
    "# Additional documentation and model training steps are typically required after this feature engineering process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4732054-c4b5-4a14-a34e-195262d06360",
   "metadata": {},
   "source": [
    "##### 1. Create Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa153de-b98f-40dc-8003-8ff9a1f32288",
   "metadata": {},
   "source": [
    "Instantiate the count_vectorizer and tf-idf_vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1952c1c5-3ae3-4da0-981e-02c42e82ada8",
   "metadata": {},
   "source": [
    "Vectorize & Fit the values:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333036e-c48b-4c48-b5f8-1a95f8633aaf",
   "metadata": {},
   "source": [
    "Scaling numerical data is essential in machine learning for the following reasons:\n",
    "\n",
    "1. To ensure that all features have the same units, preventing features with larger values from dominating the model.\n",
    "2. Speed up optimization algorithms like gradient descent.\n",
    "3. Regularization is sensitive to scales.\n",
    "4. For distance-based algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63546e7-f0c6-4258-8fd3-503d0b46ed04",
   "metadata": {},
   "source": [
    "##### Data Integration and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d801be52-6641-427e-97f6-30760487138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical data\n",
    "my_minmax_scaler = MinMaxScaler()\n",
    "X_train_numerical_scaled = my_minmax_scaler.fit_transform(X_train_numerical)\n",
    "X_val_numerical_scaled = my_minmax_scaler.transform(X_val_numerical)\n",
    "\n",
    "# Convert scaled numerical data to DataFrames\n",
    "X_train_numerical_scaled_df = pd.DataFrame(X_train_numerical_scaled, columns=X_train_numerical.columns)\n",
    "X_val_numerical_scaled_df = pd.DataFrame(X_val_numerical_scaled, columns=X_val_numerical.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79962bc-dd37-4203-aa22-287d219ba805",
   "metadata": {},
   "source": [
    "Prepare the dataframes for concatenation by resetting their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81e22ee3-0cd6-4cf7-a52d-909600da120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the indices for all DataFrames\n",
    "X_train_Assessor_Remarks_df = X_train_Assessor_Remarks_df.reset_index(drop=True)\n",
    "X_train_OPM_remarks_df = X_train_OPM_remarks_df.reset_index(drop=True)\n",
    "X_train_numerical_scaled_df = X_train_numerical_scaled_df.reset_index(drop=True)\n",
    "X_train_coded = X_train_coded.reset_index(drop=True)\n",
    "\n",
    "X_val_Assessor_Remarks_df = X_val_Assessor_Remarks_df.reset_index(drop=True)\n",
    "X_val_OPM_remarks_df = X_val_OPM_remarks_df.reset_index(drop=True)\n",
    "X_val_numerical_scaled_df = X_val_numerical_scaled_df.reset_index(drop=True)\n",
    "X_val_coded = X_val_coded.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de1bcd-b297-47f7-b9b6-8e3d88b17478",
   "metadata": {},
   "source": [
    "Combining Data Components for Training and Validation Sets: Merging Textual, Numerical, and Dummy-Coded Categorical Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b35dc46d-a80f-4ee3-869c-935d7830676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate data for training and validation sets\n",
    "X_train_remarks_merged = pd.concat([X_train_Assessor_Remarks_df, X_train_OPM_remarks_df], axis=1)\n",
    "X_train_all_merged = pd.concat([X_train_remarks_merged, X_train_numerical_scaled_df, X_train_coded], axis=1)\n",
    "\n",
    "X_val_remarks_merged = pd.concat([X_val_Assessor_Remarks_df, X_val_OPM_remarks_df], axis=1)\n",
    "X_val_all_merged = pd.concat([X_val_remarks_merged, X_val_numerical_scaled_df, X_val_coded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e959142-b2e4-4c40-851b-4a3359a90fa6",
   "metadata": {},
   "source": [
    "Print the shape of the final merged data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6d2d01a-5e7f-4f22-ac7c-67f64137305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after merging all components (train set): (1148, 1005)\n",
      "Shape after merging all components (val set): (1148, 1005)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the merged data\n",
    "print(\"Shape after merging all components (train set):\", X_train_all_merged.shape)\n",
    "print(\"Shape after merging all components (val set):\", X_val_all_merged.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3e615c-9e11-41ce-b677-1998379bcbc9",
   "metadata": {},
   "source": [
    "Calculate the total count of missing values in the merged training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf5fe244-9932-446d-8165-32bfad0c849c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing values in X_train_all_merged: 0\n",
      "Total missing values in X_val_all_merged: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "train_missing_values = X_train_all_merged.isna().sum().sum()\n",
    "val_missing_values = X_val_all_merged.isna().sum().sum()\n",
    "\n",
    "# Display the total count of missing values in both datasets\n",
    "print(f\"Total missing values in X_train_all_merged: {train_missing_values}\")\n",
    "print(f\"Total missing values in X_val_all_merged: {val_missing_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c4d8c1-3b8b-4358-ab76-fd31f5c2ac13",
   "metadata": {},
   "source": [
    "Perform a sanity check by examining data types since there are numerous columns, and .info() is not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db49e6dc-4b40-488e-be66-3e1c0a574068",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types in Merged Training Dataset:\n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
      "\n",
      "[1148 rows x 0 columns]\n",
      "\n",
      "Data Types in Merged Validation Dataset:\n",
      " Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, ...]\n",
      "\n",
      "[1148 rows x 0 columns]\n",
      "\n",
      "Total missing values in Merged Validation Dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Check the data types in the merged training dataset\n",
    "data_types_train = X_train_all_merged.select_dtypes(include=['object'])\n",
    "print(\"Data Types in Merged Training Dataset:\\n\", data_types_train)\n",
    "\n",
    "# Check the data types in the merged validation dataset\n",
    "data_types_val = X_val_all_merged.select_dtypes(include=['object'])\n",
    "print(\"\\nData Types in Merged Validation Dataset:\\n\", data_types_val)\n",
    "\n",
    "# Check for missing values in the merged validation dataset\n",
    "val_missing_values = X_val_all_merged.isna().sum().sum()\n",
    "print(f\"\\nTotal missing values in Merged Validation Dataset: {val_missing_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f00f9d26-2ae9-4c93-97eb-10812973a7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of problematic rows: []\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Check for NaN or infinity values in X_val_all_merged\n",
    "problematic_rows = np.isnan(X_val_all_merged).any(axis=1) | np.isinf(X_val_all_merged).any(axis=1)\n",
    "\n",
    "# Get the indices of the rows with problematic values\n",
    "problematic_row_indices = np.where(problematic_rows)[0]\n",
    "\n",
    "# Display the indices of the problematic rows\n",
    "print(\"Indices of problematic rows:\", problematic_row_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129400c5-d3ad-4fb4-9fcf-c95c7ff81626",
   "metadata": {},
   "source": [
    "### Lasso Baseline Model & Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e82dba8-a51f-4866-91d7-517fa13cdd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) score on the training data: 0.5709270591499089\n",
      "R-squared (R2) score on the validation data: 0.3624268519128706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haesunjung/anaconda3/envs/nlp_modelling/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.741e+13, tolerance: 4.639e+10\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    }
   ],
   "source": [
    "# Baseline Lasso Model\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Create a Lasso model with alpha (regularization strength) set to 1.0\n",
    "lasso_model = Lasso(alpha=1.0)\n",
    "\n",
    "# Fit the Lasso model to the training data\n",
    "lasso_model.fit(X_train_all_merged, y_train)\n",
    "\n",
    "# Calculate the R-squared (R2) score for the training data\n",
    "train_score = lasso_model.score(X_train_all_merged, y_train)\n",
    "print(f\"R-squared (R2) score on the training data: {train_score}\")\n",
    "\n",
    "# Calculate the R-squared (R2) score for the validation data\n",
    "val_score = lasso_model.score(X_val_all_merged, y_val)\n",
    "print(f\"R-squared (R2) score on the validation data: {val_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e33d2a-7f17-41d9-9e57-40275d99ee30",
   "metadata": {},
   "source": [
    "Estimator + random search for Lasso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5fc08954-70a4-401e-b3a9-77a1b45a4ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "R-squared (R2) score on the training data: 0.4324375063562932\n",
      "R-squared (R2) score on the validation data: 0.18754913702653375\n",
      "Best Parameters: {'scaling': RobustScaler(), 'reduce_dim__n_components': 1, 'reduce_dim': PCA(n_components=1), 'model__alpha': 100, 'model': Lasso(alpha=100)}\n",
      "Best Estimator: Pipeline(steps=[('scaling', RobustScaler()),\n",
      "                ('reduce_dim', PCA(n_components=1)),\n",
      "                ('model', Lasso(alpha=100))])\n"
     ]
    }
   ],
   "source": [
    "# Define a pipeline with multiple components: scaling, dimensionality reduction, and regression model\n",
    "estimators = [\n",
    "    ('scaling', StandardScaler()),  # Scale features\n",
    "    ('reduce_dim', PCA()),          # Reduce dimensionality using PCA\n",
    "    ('model', LinearRegression())   # Use Linear Regression as the base model\n",
    "]\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_dist = [\n",
    "    {\n",
    "        'scaling': [StandardScaler(), RobustScaler(), MinMaxScaler()],  # Scaling options\n",
    "        'reduce_dim': [PCA()],          # Use PCA for dimensionality reduction\n",
    "        'reduce_dim__n_components': [1, 100, 10],  # Number of PCA components\n",
    "        'model': [Lasso()],             # Lasso regression as the model\n",
    "        'model__alpha': [0.1, 1, 10, 100]  # Alpha values for Lasso\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# Create a Randomized Search CV object with 10 iterations, 5-fold cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10, \n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the random search to the training data to find the best hyperparameters\n",
    "random_search.fit(X_train_all_merged, y_train)\n",
    "\n",
    "# Get the best estimator with optimized hyperparameters\n",
    "best_estimator = random_search.best_estimator_\n",
    "\n",
    "# Use the best estimator to make predictions on the training and validation data\n",
    "y_pred_train = best_estimator.predict(X_train_all_merged)\n",
    "y_pred_val = best_estimator.predict(X_val_all_merged)\n",
    "\n",
    "# Calculate the R-squared (R2) scores for training and validation data\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_val = r2_score(y_val, y_pred_val)\n",
    "\n",
    "print(f\"R-squared (R2) score on the training data: {r2_train}\")\n",
    "print(f\"R-squared (R2) score on the validation data: {r2_val}\")\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Estimator:\", best_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcb739c-56b3-48e2-ae15-29b7f7a6447d",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor Baseline Mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41d6c1a9-b328-408b-9358-df7c02aff507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) Score for Training Data: 0.3062033853154149\n",
      "R-squared (R2) Score for Validation Data: 0.17132077786390998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a DecisionTreeRegressor model with specified hyperparameters\n",
    "# Hyperparameters: max_depth, min_samples_split, min_samples_leaf, and random_state\n",
    "DT_regressor_model = DecisionTreeRegressor(\n",
    "    max_depth=20,\n",
    "    min_samples_split=100,\n",
    "    min_samples_leaf=100,\n",
    "    random_state=22\n",
    ")\n",
    "\n",
    "# Fit the DecisionTreeRegressor model to the training data\n",
    "DT_regressor_model.fit(X_train_all_merged, y_train)\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_pred_train = DT_regressor_model.predict(X_train_all_merged)\n",
    "\n",
    "# Calculate the R-squared (R2) score for the training data\n",
    "DT_train_score = DT_regressor_model.score(X_train_all_merged, y_train)\n",
    "\n",
    "print(f\"R-squared (R2) Score for Training Data: {DT_train_score}\")\n",
    "\n",
    "# Calculate the R-squared (R2) score for the validation data\n",
    "DT_val_score = DT_regressor_model.score(X_val_all_merged, y_val)\n",
    "print(f\"R-squared (R2) Score for Validation Data: {DT_val_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca9b6b4-72b9-48d4-a52e-a5faf13f08e9",
   "metadata": {},
   "source": [
    "### KNN baseline model & Random Search Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e53bac3-052f-4021-b989-aa91a14f548a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) Score for Training Data: 0.999999999998265\n",
      "R-squared (R2) Score for Validation Data: 0.6557134839536438\n"
     ]
    }
   ],
   "source": [
    "# Create a KNeighborsRegressor instance with specified hyperparameters\n",
    "# Hyperparameters: n_neighbors, weights, and leaf_size\n",
    "KNN_regressor_model = KNeighborsRegressor(\n",
    "    n_neighbors=30,\n",
    "    weights=\"distance\",\n",
    "    leaf_size=100\n",
    ")\n",
    "\n",
    "# Fit the KNeighborsRegressor model to the training data\n",
    "KNN_regressor_model.fit(X_train_all_merged, y_train)\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_pred_train = KNN_regressor_model.predict(X_train_all_merged)\n",
    "\n",
    "# Calculate the R-squared (R2) score for the training data\n",
    "KNN_train_score = KNN_regressor_model.score(X_train_all_merged, y_train)\n",
    "\n",
    "print(f\"R-squared (R2) Score for Training Data: {KNN_train_score}\")\n",
    "\n",
    "# Calculate the R-squared (R2) score for the validation data\n",
    "KNN_val_score = KNN_regressor_model.score(X_val_all_merged, y_val)\n",
    "print(f\"R-squared (R2) Score for Validation Data: {KNN_val_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c4cc03d-8cac-41df-a39e-d0a2b3980c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    }
   ],
   "source": [
    "# Create an estimator using a pipeline with data preprocessing steps (scaling, dimension reduction) and KNeighborsRegressor as the model.\n",
    "estimators = [\n",
    "    ('scaling', StandardScaler()),\n",
    "    ('reduce_dim', PCA()),\n",
    "    ('model', KNeighborsRegressor())\n",
    "]\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_dist = [\n",
    "    {\n",
    "        'scaling': [StandardScaler(), RobustScaler(), MinMaxScaler()],\n",
    "        'reduce_dim': [PCA()],\n",
    "        'reduce_dim__n_components': [1, 100, 10],\n",
    "        'model': [KNeighborsRegressor()],\n",
    "        'model__n_neighbors': [3, 10, 30, 50],\n",
    "        'model__weights': ['uniform', 'distance'],\n",
    "        'model__leaf_size': [10, 50, 100],\n",
    "        'model__n_jobs': [-1]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Perform a randomized search with cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "random_search.fit(X_train_all_merged, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_estimator = random_search.best_estimator_\n",
    "\n",
    "# Make predictions on the training data and validation data using the best estimator\n",
    "y_pred_train_KNN_estimator = best_estimator.predict(X_train_all_merged)\n",
    "y_pred_val_KNN_estimator = best_estimator.predict(X_val_all_merged)\n",
    "\n",
    "# Calculate the R-squared (R2) score for both training and validation data\n",
    "r2_train_KNN_estimator = r2_score(y_train, y_pred_train_KNN_estimator)\n",
    "r2_val_KNN_estimator = r2_score(y_val, y_pred_val_KNN_estimator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f44dfed3-74ea-477c-ba5a-502992fd9d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (R2) score on the training data: 0.4324375063562932\n",
      "R-squared (R2) score on the validation data: 0.18754913702653375\n",
      "Best Parameters: {'scaling': RobustScaler(), 'reduce_dim__n_components': 1, 'reduce_dim': PCA(n_components=1), 'model__weights': 'distance', 'model__n_neighbors': 10, 'model__n_jobs': -1, 'model__leaf_size': 10, 'model': KNeighborsRegressor(leaf_size=10, n_jobs=-1, n_neighbors=10, weights='distance')}\n",
      "Best Estimator: Pipeline(steps=[('scaling', RobustScaler()),\n",
      "                ('reduce_dim', PCA(n_components=1)),\n",
      "                ('model',\n",
      "                 KNeighborsRegressor(leaf_size=10, n_jobs=-1, n_neighbors=10,\n",
      "                                     weights='distance'))])\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.4s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=Lasso(), model__alpha=1, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=MinMaxScaler(); total time=   0.4s\n",
      "[CV] END model=Lasso(), model__alpha=1, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=3, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.7s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=100, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.9s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.9s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=100, model__n_jobs=-1, model__n_neighbors=3, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.9s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.7s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=Lasso(), model__alpha=10, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.4s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=Lasso(), model__alpha=1, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=MinMaxScaler(); total time=   0.4s\n",
      "[CV] END model=Lasso(), model__alpha=1, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=3, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.7s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=100, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.9s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.6s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=100, model__n_jobs=-1, model__n_neighbors=3, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.8s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=Lasso(), model__alpha=100, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=RobustScaler(); total time=   0.8s\n",
      "[CV] END model=Lasso(), model__alpha=100, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.6s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=3, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.7s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.4s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.9s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=3, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=100, model__n_jobs=-1, model__n_neighbors=3, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.8s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.6s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.4s\n",
      "[CV] END model=Lasso(), model__alpha=100, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=RobustScaler(); total time=   0.8s\n",
      "[CV] END model=Lasso(), model__alpha=100, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.6s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=RobustScaler(); total time=   0.7s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.4s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.5s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.6s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=3, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=100, model__n_jobs=-1, model__n_neighbors=3, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.9s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.7s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.4s\n",
      "[CV] END model=Lasso(), model__alpha=100, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=RobustScaler(); total time=   0.8s\n",
      "[CV] END model=Lasso(), model__alpha=100, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.6s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.5s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=RobustScaler(); total time=   0.7s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.5s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.5s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=3, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=100, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=RobustScaler(); total time=   0.6s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=3, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.6s\n",
      "[CV] END model=Lasso(), model__alpha=100, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=RobustScaler(); total time=   0.9s\n",
      "[CV] END model=Lasso(), model__alpha=100, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.6s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=MinMaxScaler(); total time=   0.2s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=3, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.7s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=100, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.9s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.6s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.8s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=RobustScaler(); total time=   0.6s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=3, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.6s\n",
      "[CV] END model=Lasso(), model__alpha=100, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=RobustScaler(); total time=   0.9s\n",
      "[CV] END model=Lasso(), model__alpha=1, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=Lasso(), model__alpha=100, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.5s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=100, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.9s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.9s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.8s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=100, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.7s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.4s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.2s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.4s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=Lasso(), model__alpha=100, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.2s\n",
      "[CV] END model=Lasso(), model__alpha=100, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.5s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=RobustScaler(); total time=   0.8s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   1.0s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=3, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=100, model__n_jobs=-1, model__n_neighbors=3, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.8s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=3, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.6s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=Lasso(), model__alpha=10, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.4s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=Lasso(), model__alpha=100, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.2s\n",
      "[CV] END model=Lasso(), model__alpha=100, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=MinMaxScaler(); total time=   0.2s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.5s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=RobustScaler(); total time=   0.7s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.5s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.5s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.8s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=100, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.4s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=RobustScaler(); total time=   0.6s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=3, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.6s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=Lasso(), model__alpha=10, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.4s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=Lasso(), model__alpha=1, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=MinMaxScaler(); total time=   0.4s\n",
      "[CV] END model=Lasso(), model__alpha=1, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=3, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.7s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.4s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.5s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.6s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.8s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=100, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.4s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.6s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=3, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.6s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.4s\n",
      "[CV] END model=Lasso(), model__alpha=10, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=Lasso(), model__alpha=1, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=MinMaxScaler(); total time=   0.4s\n",
      "[CV] END model=Lasso(), model__alpha=1, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.5s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=100, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.9s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.9s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=3, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=100, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=RobustScaler(); total time=   0.6s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.4s\n",
      "[CV] END model=Lasso(), model__alpha=0.1, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=Lasso(), model__alpha=10, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.4s\n",
      "[CV] END model=Lasso(), model__alpha=100, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=RobustScaler(); total time=   0.6s\n",
      "[CV] END model=Lasso(), model__alpha=1, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=MinMaxScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=RobustScaler(); total time=   0.7s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=1, scaling=MinMaxScaler(); total time=   0.4s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.5s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.6s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.3s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=RobustScaler(); total time=   0.8s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=RobustScaler(); total time=   0.6s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=10, model__n_jobs=-1, model__n_neighbors=10, model__weights=uniform, reduce_dim=PCA(), reduce_dim__n_components=10, scaling=StandardScaler(); total time=   0.4s\n",
      "[CV] END model=KNeighborsRegressor(), model__leaf_size=50, model__n_jobs=-1, model__n_neighbors=50, model__weights=distance, reduce_dim=PCA(), reduce_dim__n_components=100, scaling=StandardScaler(); total time=   0.4s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"R-squared (R2) score on the training data: {r2_train}\")\n",
    "print(f\"R-squared (R2) score on the validation data: {r2_val}\")\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Estimator:\", best_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a188fe2c-0e97-4203-a4a2-1d5aa953e9e0",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec537f9-fba0-4ffb-a398-f3acc6f9e33f",
   "metadata": {
    "id": "acc4c5da"
   },
   "source": [
    "The analysis reveals that none of the models achieved a satisfactory level of performance in capturing the underlying patterns within the variance of the data, whether applied to the training dataset or the validation dataset. This outcome is particularly noteworthy, given that one of the model features was \"Assessed Value,\" which, in principle, should exhibit a correlation with the sale price. Had any of the models exhibited significant R-squared (R^2) values, the subsequent steps would have entailed further refinement and optimization, possibly including ensemble techniques such as model bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517647ad-3869-4c79-b328-8555871705bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nlp_modelling",
   "language": "python",
   "name": "nlp_modelling"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
